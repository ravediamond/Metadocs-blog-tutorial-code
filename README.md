# Metadocs Tutorials Repository

This repository contains tutorial code and resources linked to the Metadocs blog. Each folder corresponds to a specific tutorial, providing hands-on examples and insights into deploying advanced AI applications.

## Folders

### RAG-pipeline-langchain-openai

This folder contains resources for deploying a Retrieval-Augmented Generation (RAG) application using Langchain, Streamlit, and OpenAI.

**Article:** [Deploy a RAG application with Langchain, Streamlit, and OpenAI in 10 minutes](https://www.metadocs.co/2024/03/26/deploy-a-rag-application-with-langchain-streamlit-and-openai-in-10-min/)

#### Prerequisites:
- Python environment with pipenv
- OpenAI API key
- Basic understanding of LLMs, RAG pipelines, embeddings, and vector stores

#### Key Concepts Covered:
- Building a RAG pipeline
- Utilizing Langchain for application development
- Implementing Streamlit for web application deployment
- Handling embeddings and vector stores for data retrieval

### deploy-llm-sagemaker-endpoint

This folder contains the necessary code and instructions to deploy an LLM using AWS Sagemaker and Hugging Face.

**Article:** [Deploy a LLM on AWS in 5 minutes](https://www.metadocs.co/2024/03/19/deploy-a-llm-on-aws-in-5-min/)

#### Prerequisites:
- An AWS account with admin permissions
- AWS CLI configured on your machine
- Familiarity with Jupyter Lab, AWS Sagemaker, and Hugging Face's TGI
- Basic knowledge of Python and virtual environments

#### Key Concepts Covered:
- Setting up a Jupyter Lab environment with AWS Sagemaker
- Deploying Mistral 7B model on a Sagemaker endpoint
- Interacting with the deployed model through Jupyter notebook
- Properly deleting the Sagemaker endpoint to avoid additional charges
