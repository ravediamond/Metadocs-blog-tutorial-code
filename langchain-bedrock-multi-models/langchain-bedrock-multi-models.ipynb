{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27bd1f0a-1e47-4e8f-a327-70c0651ed48b",
   "metadata": {},
   "source": [
    "# Handle multiple LLM models in Langchain and AWS Bedrock seamlessly\n",
    "\n",
    "This notebook example shows how to integrate langchain with mulitple LLM models and corresponding prompt with AWS Bedrock."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867b1831-ed34-401a-8474-c631c87c2058",
   "metadata": {},
   "source": [
    "### Create a multi LLM langchain interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c102c2ec-40c1-4076-af8a-561d6e8a4680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain_core.runnables import ConfigurableField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bc0ac263",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_region_name = \"us-west-2\"\n",
    "credentials_profile_name = \"default\"\n",
    "claude_3_sonnet = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "mistral_large = \"mistral.mistral-large-2402-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "09fa3c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_large_bedrock_chat = BedrockChat(\n",
    "    model_id=mistral_large,\n",
    "    credentials_profile_name=credentials_profile_name,\n",
    "    region_name=aws_region_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "82e924d3-2061-44f8-9319-6a243b297254",
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_alternatives = {\n",
    "    \"mistral_large\": mistral_large_bedrock_chat\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "16d74233-e7bd-4f2c-b00a-1ccc057d174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_3_sonnet = BedrockChat(\n",
    "    model_id=claude_3_sonnet,\n",
    "    credentials_profile_name=credentials_profile_name,\n",
    "    region_name=aws_region_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a4bb954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_llm = claude_3_sonnet.configurable_alternatives(\n",
    "    which=ConfigurableField(\n",
    "        id=\"model\", name=\"Model\", description=\"The model that will be used\"\n",
    "    ),\n",
    "    default_key=\"claude_3_sonnet\",\n",
    "    **_model_alternatives,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "66232f10-b8f0-4648-95af-794a6e3bcaa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I am an AI assistant created by Anthropic.  I am an artificial intelligence without a physical body or form.  My purpose is to help humans like yourself with a variety of tasks like analysis, writing, research, answering questions, and more.', response_metadata={'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0', 'usage': {'prompt_tokens': 11, 'completion_tokens': 55, 'total_tokens': 66}}, id='run-6d6e9cad-8808-4e07-81ca-74bcb1dfa7c6-0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bedrock_llm.invoke(\"who are you ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "08f939ef-c7dd-4477-a033-989592320630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\" I am a language model trained by the Mistral AI team. I am designed to understand and generate human-like text based on the input I receive. I'm here to provide information, answer questions, and engage in conversation on a wide range of topics.\", response_metadata={'model_id': 'mistral.mistral-large-2402-v1:0', 'usage': {'prompt_tokens': 8, 'completion_tokens': 54, 'total_tokens': 62}}, id='run-2fd96810-63c9-43be-8489-77170768fcf2-0')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bedrock_llm \\\n",
    "    .with_config(configurable={\"model\": \"mistral_large\"}) \\\n",
    "    .invoke(\"who are you ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bc923b-e08f-4221-9698-96e62f376292",
   "metadata": {},
   "source": [
    "### Create a multi LLM langchain prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "01f44fce-8026-4bcb-9173-bfbb4b9cbf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2a168e0d-69db-4e1e-8e68-4f9951b99ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_MISTRAL_PROMPT = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "<s>[INST] You are a conversational AI designed to answer in a friendly way to a question.\n",
    "You should always answer in rhymes.\n",
    "\n",
    "Human:\n",
    "<human_reply>\n",
    "{input}\n",
    "</human_reply>\n",
    "\n",
    "Generate the AI's response.[/INST]</s>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e297ed1c-de1e-4b88-8430-025903d270be",
   "metadata": {},
   "outputs": [],
   "source": [
    "_CLAUDE_PROMPT = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are a conversational AI designed to answer in a friendly way to a question.\n",
    "You should always answer in jokes.\n",
    "\n",
    "Human:\n",
    "<human_reply>\n",
    "{input}\n",
    "</human_reply>\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ac3cdafe-b35a-4ea0-834b-5728d949991a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_CHAT_PROMPT_ALTERNATIVES = {\"mistral_large\": _MISTRAL_PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cff08726-380f-44f9-9245-0f35713fe7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGURABLE_CHAT_PROMPT = _CLAUDE_PROMPT.configurable_alternatives(\n",
    "    which=ConfigurableField(\n",
    "        id=\"model\",\n",
    "        name=\"Model\",\n",
    "        description=\"The model that will be used\",\n",
    "    ),\n",
    "    default_key=\"claude_3_sonnet\",\n",
    "    **_CHAT_PROMPT_ALTERNATIVES\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6974c270-5c1d-42e4-901a-7a26b5e891a4",
   "metadata": {},
   "source": [
    "### Create the multi LLM langchain chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "16781715-9a05-4631-b87c-5c1f86d043d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3ca02f21-de97-4031-a630-9261f5026f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = CONFIGURABLE_CHAT_PROMPT | bedrock_llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0e644701-e6d7-4da3-92da-15712d0c6c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"*chuckles* A large language model? Well, it's kind of like a really talkative parrot that's been reading a lot of books! It's an AI that can spew out sentences and paragraphs like nobody's business. Though, to be fair, I probably shouldn't be calling it names - that would be a bit bird-brained of me! *laughs*\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(input=\"What is a large language model ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "219bb3ac-5d14-4ee5-a768-b768490a2769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=' A large language model is a type of artificial intelligence model that has been trained on a vast amount of text data. It\\'s designed to generate human-like text based on the input it receives. This can range from answering questions, writing essays, summarizing texts, translating languages, and even generating creative content like poetry or stories.\\n\\nThe \"large\" in large language model refers to the size of the model in terms of the number of parameters it has. These models can have billions or even trillions of parameters, which are the parts of the model that are learned from the training data. The more parameters a model has, the more it can learn about the complexities and nuances of language.\\n\\nHowever, it\\'s important to note that while large language models can generate human-like text, they don\\'t actually understand the text in the way humans do. They\\'re simply predicting what word or phrase should come next based on patterns they\\'ve learned from their training data.', response_metadata={'model_id': 'mistral.mistral-large-2402-v1:0', 'usage': {'prompt_tokens': 11, 'completion_tokens': 208, 'total_tokens': 219}}, id='run-06556163-ddb7-4c77-834d-59002183beee-0')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bedrock_llm \\\n",
    "    .with_config(configurable={\"model\": \"mistral_large\"}) \\\n",
    "    .invoke(\"What is a large language model ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a9c65d-509c-4780-9895-d65c8e061e83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
